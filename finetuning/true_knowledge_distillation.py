"""
çœŸæ­£çš„çŸ¥è¯†è’¸é¦å®ç°
åºåˆ—åŒ–æ–¹å¼ï¼šå…ˆæå–æ•™å¸ˆçŸ¥è¯†ï¼Œå†è®­ç»ƒå­¦ç”Ÿæ¨¡å‹
æ˜¾å­˜éœ€æ±‚ï¼šçº¦10GBï¼ˆé€‚åˆRTX 4090ï¼‰
"""
import os
import torch
import torch.nn.functional as F
from transformers import (
    AutoModelForCausalLM, 
    AutoTokenizer,
    TrainingArguments,
    Trainer,
    DataCollatorForLanguageModeling
)
from peft import PeftModel
from datasets import Dataset
import json
import time
from datetime import datetime
import numpy as np
import pickle
from tqdm import tqdm

print("ğŸ“ DeepCode-Analyst çœŸæ­£çŸ¥è¯†è’¸é¦")
print("=" * 50)

# ç¯å¢ƒé…ç½®
cache_dir = "/root/autodl-tmp/huggingface_cache"
os.environ['HF_HOME'] = cache_dir
os.environ['TRANSFORMERS_CACHE'] = cache_dir + "/transformers"
os.environ['HF_HUB_CACHE'] = cache_dir + "/hub"
os.environ['TOKENIZERS_PARALLELISM'] = 'false'
os.environ['HF_ENDPOINT'] = 'https://hf-mirror.com'

class KnowledgeDistillationTrainer(Trainer):
    """çœŸæ­£çš„çŸ¥è¯†è’¸é¦è®­ç»ƒå™¨"""
    
    def __init__(self, teacher_outputs, temperature=4.0, alpha=0.7, **kwargs):
        super().__init__(**kwargs)
        self.teacher_outputs = teacher_outputs  # é¢„è®¡ç®—çš„æ•™å¸ˆè¾“å‡º
        self.temperature = temperature
        self.alpha = alpha  # KDæŸå¤±æƒé‡
        self.sample_idx = 0
        
    def get_teacher_output(self, batch_size):
        """è·å–å¯¹åº”çš„æ•™å¸ˆè¾“å‡º"""
        teacher_logits = []
        for i in range(batch_size):
            if self.sample_idx < len(self.teacher_outputs):
                teacher_logits.append(self.teacher_outputs[self.sample_idx]['logits'])
                self.sample_idx += 1
            else:
                # å¾ªç¯ä½¿ç”¨
                idx = self.sample_idx % len(self.teacher_outputs)
                teacher_logits.append(self.teacher_outputs[idx]['logits'])
                self.sample_idx += 1
        
        return torch.stack(teacher_logits)
    
    def compute_loss(self, model, inputs, return_outputs=False, **kwargs):
        """è®¡ç®—çŸ¥è¯†è’¸é¦æŸå¤±"""
        labels = inputs.get("labels")
        batch_size = inputs['input_ids'].size(0)
        
        # å­¦ç”Ÿæ¨¡å‹å‰å‘ä¼ æ’­
        student_outputs = model(**inputs)
        student_logits = student_outputs.logits
        
        # è·å–é¢„è®¡ç®—çš„æ•™å¸ˆlogits
        teacher_logits = self.get_teacher_output(batch_size).to(student_logits.device)
        
        # 1. æ ‡å‡†äº¤å‰ç†µæŸå¤±ï¼ˆç¡¬æ ‡ç­¾ï¼‰
        shift_logits = student_logits[..., :-1, :].contiguous()
        shift_labels = labels[..., 1:].contiguous()
        loss_fct = torch.nn.CrossEntropyLoss(ignore_index=-100)
        hard_loss = loss_fct(shift_logits.view(-1, shift_logits.size(-1)), shift_labels.view(-1))
        
        # 2. çŸ¥è¯†è’¸é¦æŸå¤±ï¼ˆè½¯æ ‡ç­¾ï¼‰
        # ç¡®ä¿ç»´åº¦åŒ¹é…
        min_seq_len = min(shift_logits.size(1), teacher_logits.size(1) - 1)
        min_vocab_size = min(shift_logits.size(-1), teacher_logits.size(-1))
        
        if min_seq_len > 0 and min_vocab_size > 0:
            # æˆªå–åŒ¹é…çš„éƒ¨åˆ†
            student_logits_kd = shift_logits[:, :min_seq_len, :min_vocab_size]
            teacher_logits_kd = teacher_logits[:, :min_seq_len, :min_vocab_size]
            
            # è®¡ç®—è½¯æ ‡ç­¾æŸå¤±
            student_log_probs = F.log_softmax(student_logits_kd / self.temperature, dim=-1)
            teacher_probs = F.softmax(teacher_logits_kd / self.temperature, dim=-1)
            
            # KLæ•£åº¦æŸå¤±
            soft_loss = F.kl_div(
                student_log_probs, 
                teacher_probs, 
                reduction='batchmean'
            ) * (self.temperature ** 2)
        else:
            soft_loss = torch.tensor(0.0, device=student_logits.device)
        
        # 3. ç»„åˆæŸå¤±
        total_loss = (1 - self.alpha) * hard_loss + self.alpha * soft_loss
        
        # æ—¥å¿—è®°å½•
        if hasattr(self, 'state') and self.state.global_step % 5 == 0:
            print(f"Step {self.state.global_step}: Hard={hard_loss:.4f}, Soft={soft_loss:.4f}, Total={total_loss:.4f}")
        
        return (total_loss, student_outputs) if return_outputs else total_loss

def extract_teacher_knowledge():
    """ç¬¬ä¸€é˜¶æ®µï¼šæå–æ•™å¸ˆæ¨¡å‹çŸ¥è¯†"""
    print("\nğŸ“ é˜¶æ®µ1: æå–æ•™å¸ˆæ¨¡å‹çŸ¥è¯†")
    print("-" * 40)
    
    # åŠ è½½æ•™å¸ˆæ¨¡å‹
    print("ğŸ“¥ åŠ è½½æ•™å¸ˆæ¨¡å‹...")
    base_model_id = "Qwen/Qwen1.5-7B-Chat"
    adapter_path = "/root/autodl-tmp/models/deepcode-analyst-finetuned"
    
    # æ£€æŸ¥æ•™å¸ˆæ¨¡å‹
    if not os.path.exists(adapter_path):
        raise FileNotFoundError(f"æ•™å¸ˆæ¨¡å‹ä¸å­˜åœ¨: {adapter_path}")
    
    # åŠ è½½åˆ†è¯å™¨
    tokenizer = AutoTokenizer.from_pretrained(
        base_model_id, 
        trust_remote_code=True,
        cache_dir=cache_dir
    )
    if tokenizer.pad_token is None:
        tokenizer.pad_token = tokenizer.eos_token
    
    # åŠ è½½åŸºç¡€æ¨¡å‹
    print("ğŸ”§ åŠ è½½7BåŸºç¡€æ¨¡å‹...")
    base_model = AutoModelForCausalLM.from_pretrained(
        base_model_id,
        torch_dtype=torch.bfloat16,
        device_map="auto",
        trust_remote_code=True,
        cache_dir=cache_dir
    )
    
    # åŠ è½½LoRAé€‚é…å™¨
    print("ğŸ¯ åŠ è½½å¾®è°ƒé€‚é…å™¨...")
    teacher_model = PeftModel.from_pretrained(base_model, adapter_path)
    teacher_model.eval()
    
    print("âœ… æ•™å¸ˆæ¨¡å‹åŠ è½½å®Œæˆ")
    
    # æ˜¾å­˜çŠ¶æ€
    allocated = torch.cuda.memory_allocated(0) / 1024**3
    print(f"ğŸ“Š å½“å‰æ˜¾å­˜ä½¿ç”¨: {allocated:.2f}GB")
    
    # åŠ è½½è®­ç»ƒæ•°æ®
    print("\nğŸ“š å¤„ç†è®­ç»ƒæ•°æ®...")
    with open('./training_data.jsonl', 'r', encoding='utf-8') as f:
        data = [json.loads(line) for line in f]
    
    print(f"ğŸ“‹ æ€»æ ·æœ¬æ•°: {len(data)}")
    
    # æå–æ•™å¸ˆçŸ¥è¯†
    teacher_outputs = []
    device = next(teacher_model.parameters()).device
    
    print("\nğŸ§  æå–æ•™å¸ˆæ¨¡å‹çŸ¥è¯†...")
    for i, item in enumerate(tqdm(data, desc="å¤„ç†æ ·æœ¬")):
        # æ ¼å¼åŒ–è¾“å…¥
        text = f"### æŒ‡ä»¤:\n{item['instruction']}\n\n### è¾“å…¥:\n{item['input']}\n\n### è¾“å‡º:\n{item['output']}"
        
        # åˆ†è¯
        inputs = tokenizer(
            text,
            truncation=True,
            padding=False,
            max_length=1024,
            return_tensors="pt"
        ).to(device)
        
        # è·å–æ•™å¸ˆæ¨¡å‹è¾“å‡º
        with torch.no_grad():
            outputs = teacher_model(**inputs)
            logits = outputs.logits.cpu().detach()
            
            # åŒæ—¶ç”Ÿæˆæ•™å¸ˆæ¨¡å‹çš„å®é™…å“åº”ç”¨äºå¯¹æ¯”
            generated = teacher_model.generate(
                **inputs,
                max_new_tokens=100,
                temperature=0.7,
                do_sample=True,
                pad_token_id=tokenizer.eos_token_id
            )
            
            teacher_response = tokenizer.decode(
                generated[0][inputs['input_ids'].shape[1]:], 
                skip_special_tokens=True
            )
        
        teacher_outputs.append({
            'sample_id': i,
            'instruction': item['instruction'],
            'input': item['input'],
            'expected_output': item['output'],
            'teacher_response': teacher_response,
            'logits': logits.squeeze(0),  # ç§»é™¤batchç»´åº¦
            'input_ids': inputs['input_ids'].cpu().squeeze(0)
        })
        
        # æ¸…ç†æ˜¾å­˜
        del outputs, inputs, generated
        if i % 10 == 0:
            torch.cuda.empty_cache()
    
    # ä¿å­˜æ•™å¸ˆçŸ¥è¯†
    knowledge_path = "/root/autodl-tmp/teacher_knowledge.pkl"
    print(f"\nğŸ’¾ ä¿å­˜æ•™å¸ˆçŸ¥è¯†åˆ°: {knowledge_path}")
    
    with open(knowledge_path, 'wb') as f:
        pickle.dump({
            'teacher_outputs': teacher_outputs,
            'tokenizer_config': {
                'model_id': base_model_id,
                'vocab_size': tokenizer.vocab_size,
                'pad_token_id': tokenizer.pad_token_id,
                'eos_token_id': tokenizer.eos_token_id
            },
            'extraction_info': {
                'timestamp': datetime.now().isoformat(),
                'num_samples': len(teacher_outputs),
                'teacher_model': adapter_path
            }
        }, f)
    
    # é‡Šæ”¾æ•™å¸ˆæ¨¡å‹æ˜¾å­˜
    print("ğŸ—‘ï¸ é‡Šæ”¾æ•™å¸ˆæ¨¡å‹æ˜¾å­˜...")
    del teacher_model, base_model
    torch.cuda.empty_cache()
    
    print(f"âœ… çŸ¥è¯†æå–å®Œæˆ! å¤„ç†äº† {len(teacher_outputs)} ä¸ªæ ·æœ¬")
    print(f"ğŸ“ çŸ¥è¯†æ–‡ä»¶å¤§å°: {os.path.getsize(knowledge_path) / 1024**2:.1f}MB")
    
    return knowledge_path, tokenizer

def train_student_model(knowledge_path, tokenizer):
    """ç¬¬äºŒé˜¶æ®µï¼šè®­ç»ƒå­¦ç”Ÿæ¨¡å‹"""
    print("\nğŸ‘¨â€ğŸ“ é˜¶æ®µ2: è®­ç»ƒå­¦ç”Ÿæ¨¡å‹")
    print("-" * 40)
    
    # åŠ è½½æ•™å¸ˆçŸ¥è¯†
    print("ğŸ“– åŠ è½½æ•™å¸ˆçŸ¥è¯†...")
    with open(knowledge_path, 'rb') as f:
        knowledge_data = pickle.load(f)
    
    teacher_outputs = knowledge_data['teacher_outputs']
    print(f"ğŸ“š åŠ è½½äº† {len(teacher_outputs)} ä¸ªæ•™å¸ˆæ ·æœ¬")
    
    # åŠ è½½å­¦ç”Ÿæ¨¡å‹
    print("ğŸ‘¨â€ğŸ“ åŠ è½½å­¦ç”Ÿæ¨¡å‹...")
    student_model_id = "Qwen/Qwen1.5-1.8B-Chat"
    student_model = AutoModelForCausalLM.from_pretrained(
        student_model_id,
        torch_dtype=torch.bfloat16,
        device_map="auto",
        trust_remote_code=True,
        cache_dir=cache_dir
    )
    
    print("âœ… å­¦ç”Ÿæ¨¡å‹åŠ è½½å®Œæˆ")
    
    # æ˜¾å­˜æ£€æŸ¥
    allocated = torch.cuda.memory_allocated(0) / 1024**3
    print(f"ğŸ“Š å½“å‰æ˜¾å­˜ä½¿ç”¨: {allocated:.2f}GB")
    
    # å‡†å¤‡è®­ç»ƒæ•°æ®
    print("\nğŸ“‹ å‡†å¤‡è®­ç»ƒæ•°æ®...")
    formatted_data = []
    for item in teacher_outputs:
        text = f"### æŒ‡ä»¤:\n{item['instruction']}\n\n### è¾“å…¥:\n{item['input']}\n\n### è¾“å‡º:\n{item['expected_output']}"
        formatted_data.append({"text": text})
    
    dataset = Dataset.from_list(formatted_data)
    
    def tokenize_function(examples):
        model_inputs = tokenizer(
            examples['text'],
            truncation=True,
            padding=False,
            max_length=1024
        )
        model_inputs["labels"] = model_inputs["input_ids"].copy()
        return model_inputs
    
    tokenized_dataset = dataset.map(
        tokenize_function,
        batched=True,
        remove_columns=dataset.column_names,
        desc="åˆ†è¯å¤„ç†"
    )
    
    print(f"âœ… æ•°æ®é›†å‡†å¤‡å®Œæˆ: {len(tokenized_dataset)} ä¸ªæ ·æœ¬")
    
    # é…ç½®è®­ç»ƒ
    output_dir = "/root/autodl-tmp/models/deepcode-analyst-kd"
    os.makedirs(output_dir, exist_ok=True)
    
    data_collator = DataCollatorForLanguageModeling(
        tokenizer=tokenizer,
        mlm=False,
        pad_to_multiple_of=8
    )
    
    # çŸ¥è¯†è’¸é¦è®­ç»ƒå‚æ•°
    training_args = TrainingArguments(
        output_dir=output_dir,
        per_device_train_batch_size=1,
        gradient_accumulation_steps=4,
        num_train_epochs=3,  # è’¸é¦é€šå¸¸éœ€è¦æ›´å¤šè½®æ¬¡
        learning_rate=5e-5,
        logging_steps=5,
        save_steps=20,
        warmup_steps=10,
        bf16=True,
        gradient_checkpointing=True,
        dataloader_pin_memory=True,
        remove_unused_columns=False,
        report_to=[],
        save_total_limit=2,
        max_grad_norm=1.0,
        eval_strategy="no",
        save_safetensors=True,
    )
    
    print(f"ğŸ¯ è®­ç»ƒé…ç½®:")
    print(f"   è½®æ¬¡: {training_args.num_train_epochs}")
    print(f"   æ‰¹æ¬¡å¤§å°: {training_args.per_device_train_batch_size}")
    print(f"   æ¢¯åº¦ç´¯ç§¯: {training_args.gradient_accumulation_steps}")
    print(f"   å­¦ä¹ ç‡: {training_args.learning_rate}")
    print(f"   è¾“å‡ºç›®å½•: {output_dir}")
    
    # åˆ›å»ºçŸ¥è¯†è’¸é¦è®­ç»ƒå™¨
    trainer = KnowledgeDistillationTrainer(
        teacher_outputs=teacher_outputs,
        model=student_model,
        args=training_args,
        train_dataset=tokenized_dataset,
        data_collator=data_collator,
        temperature=4.0,  # æ¸©åº¦å‚æ•°
        alpha=0.7       # 70% KDæŸå¤± + 30% ç¡¬æ ‡ç­¾æŸå¤±
    )
    
    # å¼€å§‹çŸ¥è¯†è’¸é¦è®­ç»ƒ
    print("\nğŸ”¥ å¼€å§‹çŸ¥è¯†è’¸é¦è®­ç»ƒ...")
    start_time = datetime.now()
    
    train_result = trainer.train()
    
    end_time = datetime.now()
    duration = end_time - start_time
    
    # ä¿å­˜è’¸é¦æ¨¡å‹
    print("\nğŸ’¾ ä¿å­˜è’¸é¦æ¨¡å‹...")
    trainer.save_model()
    tokenizer.save_pretrained(output_dir)
    
    # ä¿å­˜è’¸é¦ä¿¡æ¯
    distillation_info = {
        "start_time": start_time.isoformat(),
        "end_time": end_time.isoformat(),
        "duration": str(duration),
        "teacher_model": "Qwen1.5-7B + LoRAå¾®è°ƒ",
        "student_model": "Qwen1.5-1.8B",
        "training_samples": len(teacher_outputs),
        "final_loss": train_result.training_loss,
        "total_steps": train_result.global_step,
        "distillation_config": {
            "temperature": 4.0,
            "alpha": 0.7,
            "epochs": training_args.num_train_epochs
        }
    }
    
    with open(f"{output_dir}/distillation_info.json", 'w', encoding='utf-8') as f:
        json.dump(distillation_info, f, ensure_ascii=False, indent=2)
    
    print("âœ… çŸ¥è¯†è’¸é¦è®­ç»ƒå®Œæˆ!")
    print(f"â° è®­ç»ƒè€—æ—¶: {duration}")
    print(f"ğŸ“Š æœ€ç»ˆæŸå¤±: {train_result.training_loss:.4f}")
    print(f"ğŸ”„ è®­ç»ƒæ­¥æ•°: {train_result.global_step}")
    print(f"ğŸ“ æ¨¡å‹ä¿å­˜: {output_dir}")
    
    return output_dir

def test_distilled_model(model_path):
    """ç¬¬ä¸‰é˜¶æ®µï¼šæµ‹è¯•è’¸é¦æ•ˆæœ"""
    print(f"\nğŸ§ª é˜¶æ®µ3: æµ‹è¯•è’¸é¦æ¨¡å‹")
    print("-" * 40)
    
    # åŠ è½½è’¸é¦æ¨¡å‹
    print("ğŸ“¥ åŠ è½½è’¸é¦æ¨¡å‹...")
    tokenizer = AutoTokenizer.from_pretrained(model_path, trust_remote_code=True)
    model = AutoModelForCausalLM.from_pretrained(
        model_path,
        torch_dtype=torch.bfloat16,
        device_map="auto",
        trust_remote_code=True
    )
    
    print("âœ… è’¸é¦æ¨¡å‹åŠ è½½å®Œæˆ")
    
    # æµ‹è¯•ç”¨ä¾‹
    test_cases = [
        {
            "name": "é¡¹ç›®æ¶æ„åˆ†æ",
            "prompt": """### æŒ‡ä»¤:
æ ¹æ®æä¾›çš„ä»£ç åº“ç»“æ„åˆ†æï¼Œç”Ÿæˆä¸€ä»½å…³äºé¡¹ç›®æ•´ä½“æ¶æ„çš„æŠ€æœ¯æŠ¥å‘Š

### è¾“å…¥:
{"project_name": "Analytics-Platform", "components": ["data-ingestion", "processing", "visualization"], "key_files": ["pipeline.py", "analyzer.py", "dashboard.js"]}

### è¾“å‡º:
"""
        },
        {
            "name": "å‡½æ•°æ€§èƒ½åˆ†æ", 
            "prompt": """### æŒ‡ä»¤:
è¯·æ·±å…¥åˆ†ææŒ‡å®šPythonå‡½æ•°çš„å†…éƒ¨å®ç°é€»è¾‘ã€å‚æ•°å’Œæ€§èƒ½ç‰¹å¾

### è¾“å…¥:
{"function_name": "optimize_query", "parameters": ["sql_query", "cache_enabled", "timeout"], "complexity": "high", "performance_critical": true}

### è¾“å‡º:
"""
        },
        {
            "name": "ä»£ç ä¼˜åŒ–å»ºè®®",
            "prompt": """### æŒ‡ä»¤:
åˆ†æç»™å®šçš„ä»£ç ç‰‡æ®µï¼Œè¯†åˆ«å…¶ä¸­å¯èƒ½å­˜åœ¨çš„æ€§èƒ½ç“¶é¢ˆï¼Œå¹¶æä¾›è¯¦ç»†çš„ä¼˜åŒ–å»ºè®®

### è¾“å…¥:
{"code_snippet": "def process_large_dataset(data):\\n    results = []\\n    for item in data:\\n        processed = expensive_transform(item)\\n        results.append(processed)\\n    return results", "context": "å¤„ç†ç™¾ä¸‡çº§æ•°æ®æ—¶æ€§èƒ½è¾ƒæ…¢"}

### è¾“å‡º:
"""
        }
    ]
    
    print(f"ğŸ¯ è¿è¡Œ {len(test_cases)} ä¸ªæµ‹è¯•ç”¨ä¾‹...")
    
    for i, test_case in enumerate(test_cases, 1):
        print(f"\nğŸ“ æµ‹è¯• {i}: {test_case['name']}")
        print("-" * 30)
        
        device = next(model.parameters()).device
        inputs = tokenizer(test_case['prompt'], return_tensors="pt").to(device)
        
        start_time = time.time()
        
        with torch.no_grad():
            outputs = model.generate(
                **inputs,
                max_new_tokens=200,
                temperature=0.7,
                do_sample=True,
                pad_token_id=tokenizer.eos_token_id,
                eos_token_id=tokenizer.eos_token_id
            )
        
        inference_time = time.time() - start_time
        
        response = tokenizer.decode(outputs[0][inputs['input_ids'].shape[1]:], skip_special_tokens=True)
        
        print(f"â±ï¸  æ¨ç†æ—¶é—´: {inference_time:.3f}ç§’")
        print(f"ğŸ“„ å“åº”é•¿åº¦: {len(response)}å­—ç¬¦")
        print(f"ğŸ’¬ ç”Ÿæˆå†…å®¹:")
        print(response[:200] + "..." if len(response) > 200 else response)
        
        # ç®€å•è´¨é‡è¯„ä¼°
        quality_indicators = 0
        if len(response) > 50:
            quality_indicators += 1
        if any(word in response for word in ['åˆ†æ', 'æŠ¥å‘Š', 'å»ºè®®', 'ä¼˜åŒ–', 'æ€§èƒ½']):
            quality_indicators += 1
        if any(marker in response for marker in ['###', '**', '1.', '2.', '-', '*']):
            quality_indicators += 1
        
        quality_score = quality_indicators / 3
        print(f"ğŸ“Š è´¨é‡è¯„åˆ†: {quality_score:.1%}")
        print("âœ… æµ‹è¯•é€šè¿‡" if quality_score >= 0.6 else "âš ï¸ éœ€è¦æ”¹è¿›")

def main():
    """ä¸»å‡½æ•°ï¼šæ‰§è¡Œå®Œæ•´çš„çŸ¥è¯†è’¸é¦æµç¨‹"""
    print("ğŸš€ å¼€å§‹çœŸæ­£çš„çŸ¥è¯†è’¸é¦æµç¨‹")
    
    # æ£€æŸ¥ç¯å¢ƒ
    if not torch.cuda.is_available():
        print("âŒ éœ€è¦GPUæ”¯æŒ")
        return
    
    gpu_memory = torch.cuda.get_device_properties(0).total_memory / 1024**3
    print(f"ğŸ® GPU: {torch.cuda.get_device_name(0)}")
    print(f"ğŸ’¾ æ˜¾å­˜: {gpu_memory:.1f}GB")
    
    if gpu_memory < 20:
        print("âš ï¸ å»ºè®®ä½¿ç”¨20GB+æ˜¾å­˜çš„GPUä»¥è·å¾—æœ€ä½³æ•ˆæœ")
    
    try:
        # ç¬¬ä¸€é˜¶æ®µï¼šæå–æ•™å¸ˆçŸ¥è¯†
        knowledge_path, tokenizer = extract_teacher_knowledge()
        
        # ç¬¬äºŒé˜¶æ®µï¼šè®­ç»ƒå­¦ç”Ÿæ¨¡å‹
        model_path = train_student_model(knowledge_path, tokenizer)
        
        # ç¬¬ä¸‰é˜¶æ®µï¼šæµ‹è¯•è’¸é¦æ•ˆæœ
        test_distilled_model(model_path)
        
        print(f"\nğŸ‰ çŸ¥è¯†è’¸é¦å®Œæˆ!")
        print(f"ğŸ“ è’¸é¦æ¨¡å‹: {model_path}")
        print(f"ğŸ“ æ•™å¸ˆçŸ¥è¯†: {knowledge_path}")
        
        print(f"\nğŸ“‹ åç»­æ“ä½œ:")
        print(f"1. æŸ¥çœ‹æ¨¡å‹: ls -la {model_path}")
        print(f"2. æ‰“åŒ…ä¸‹è½½: tar -czf distilled_model.tar.gz {model_path}")
        print(f"3. è¯¦ç»†æµ‹è¯•: python test_distilled_model.py")
        
    except Exception as e:
        print(f"âŒ çŸ¥è¯†è’¸é¦å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    main()
