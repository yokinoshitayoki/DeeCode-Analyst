# DeepCode-Analyst 完整部署指南

## 模型结构说明

### 当前下载的内容
您通过scp下载的是：
- **LoRA适配器** (~100MB) - 我们训练的专业知识
- **配置文件** - 模型参数设置

### 还需要的基础模型
要完整运行，还需要：
- **Qwen1.5-0.5B-Chat** 基础模型 (~1.2GB)
- **Qwen1.5-7B-Chat** 基础模型 (~14GB) - 如果使用教师模型

## 完整部署方案

### 方案1：本地自动下载基础模型（推荐）

```python
# 运行时自动下载基础模型
from transformers import AutoTokenizer, AutoModelForCausalLM
from peft import PeftModel

# 1. 自动下载基础模型（首次运行时）
base_model = AutoModelForCausalLM.from_pretrained(
    "Qwen/Qwen1.5-0.5B-Chat",
    trust_remote_code=True
)

# 2. 加载您下载的LoRA适配器
model = PeftModel.from_pretrained(
    base_model, 
    "./cloud_data/models/student_0.5b"
)
```

### 方案2：手动下载基础模型到云端缓存

如果您想完全离线使用，需要额外下载基础模型：

```bash
# 下载基础模型缓存（如果云端有的话）
scp -P 18812 -r root@connect.bjb1.seetacloud.com:/root/autodl-tmp/huggingface_cache/transformers/models--Qwen--Qwen1.5-0.5B-Chat/ "D:\DeepCode-Analyst 基于多智能体与图谱推理的开源项目深度解析与技术问答系统\cloud_data\models\base_models\Qwen1.5-0.5B-Chat\"
```

## 推荐使用方式

### 最简单的方法：
1. **保持当前下载的LoRA适配器**
2. **首次运行时让程序自动下载基础模型**
3. **基础模型会缓存到本地，后续无需重新下载**

### 优势：
- **节省手动下载时间**
- **自动管理模型版本**
- **智能缓存机制**
- **支持国内镜像加速**

## 实际使用代码

创建一个完整的使用示例：

```python
import os
import torch
from transformers import AutoTokenizer, AutoModelForCausalLM
from peft import PeftModel

def load_student_model():
    """加载0.5B学生模型（LoRA蒸馏版）"""
    
    print("加载0.5B学生模型...")
    
    # 1. 自动下载并加载基础模型
    print("加载基础模型 Qwen1.5-0.5B-Chat...")
    base_model = AutoModelForCausalLM.from_pretrained(
        "Qwen/Qwen1.5-0.5B-Chat",
        torch_dtype=torch.float16,
        device_map="auto",
        trust_remote_code=True
    )
    
    # 2. 加载您下载的LoRA适配器
    print("加载LoRA适配器...")
    adapter_path = "./cloud_data/models/student_0.5b"
    model = PeftModel.from_pretrained(base_model, adapter_path)
    
    # 3. 加载分词器
    tokenizer = AutoTokenizer.from_pretrained(
        "Qwen/Qwen1.5-0.5B-Chat",
        trust_remote_code=True
    )
    
    print("模型加载完成!")
    return model, tokenizer

def test_model():
    """测试模型"""
    model, tokenizer = load_student_model()
    
    # 测试代码分析
    prompt = """### 指令:
分析这段代码的性能问题

### 输入:
def slow_function(data):
    result = []
    for item in data:
        if item > 0:
            result.append(item * 2)
    return result

### 输出:
"""
    
    inputs = tokenizer(prompt, return_tensors="pt")
    
    with torch.no_grad():
        outputs = model.generate(
            **inputs,
            max_new_tokens=150,
            temperature=0.7,
            do_sample=True
        )
    
    response = tokenizer.decode(
        outputs[0][inputs['input_ids'].shape[1]:], 
        skip_special_tokens=True
    )
    
    print("模型输出:")
    print(response)

if __name__ == "__main__":
    test_model()
```

## 文件大小对比

| 组件 | 大小 | 是否已下载 | 获取方式 |
|------|------|------------|----------|
| LoRA适配器 | ~100MB | 已下载 | scp从云端 |
| 基础模型 | ~1.2GB | 未下载 | 自动下载 |
| 总计 | ~1.3GB | 部分完成 | 混合方式 |

## 总结

**您当前的状态**：
- 已有专业的代码分析能力（LoRA适配器）
- 缺少基础语言能力（基础模型）

**推荐下一步**：
- 直接运行上述Python代码
- 让程序自动下载基础模型
- 开始使用您的专业代码分析AI！

您的知识蒸馏成果（LoRA适配器）已经成功下载，这是最核心的部分！基础模型可以随时自动获取。
