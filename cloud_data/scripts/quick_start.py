"""
DeepCode-Analyst æœ¬åœ°å¿«é€Ÿå¯åŠ¨è„šæœ¬
"""
import sys
from pathlib import Path

# æ·»åŠ å½“å‰ç›®å½•åˆ°Pythonè·¯å¾„
sys.path.append(str(Path(__file__).parent))

from local_model_loader import LocalModelLoader
import json

def main():
    """ä¸»å‡½æ•°"""
    print("ğŸš€ DeepCode-Analyst æœ¬åœ°ç‰ˆå¯åŠ¨")
    print("=" * 50)
    
    # åŠ è½½é…ç½®
    try:
        with open("local_config.json", 'r', encoding='utf-8') as f:
            config = json.load(f)
        print("âœ… é…ç½®æ–‡ä»¶åŠ è½½æˆåŠŸ")
    except:
        print("âŒ é…ç½®æ–‡ä»¶åŠ è½½å¤±è´¥ï¼Œä½¿ç”¨é»˜è®¤é…ç½®")
        config = {"local_config": {"cloud_data_dir": "./cloud_data"}}
    
    # åˆ›å»ºæ¨¡å‹åŠ è½½å™¨
    loader = LocalModelLoader(config["local_config"]["cloud_data_dir"])
    
    # æ˜¾ç¤ºæ¨¡å‹çŠ¶æ€
    info = loader.get_model_info()
    print("\nğŸ“Š æ¨¡å‹çŠ¶æ€:")
    for model_name, model_info in info["models"].items():
        status = "âœ…" if model_info["exists"] else "âŒ"
        print(f"  {status} {model_name}: {model_info['size']}")
    
    # é€‰æ‹©è¦ä½¿ç”¨çš„æ¨¡å‹
    print("\nğŸ¤– é€‰æ‹©æ¨¡å‹:")
    print("1. 0.5Bå­¦ç”Ÿæ¨¡å‹ (æ¨èï¼Œè½»é‡çº§)")
    print("2. 7Bæ•™å¸ˆæ¨¡å‹ (éœ€è¦æ›´å¤šGPUå†…å­˜)")
    
    choice = input("è¯·é€‰æ‹© (1/2): ").strip()
    
    if choice == "1":
        print("\nğŸ‘¨â€ğŸ“ åŠ è½½0.5Bå­¦ç”Ÿæ¨¡å‹...")
        model, tokenizer = loader.load_student_model()
    elif choice == "2":
        print("\nğŸ“ åŠ è½½7Bæ•™å¸ˆæ¨¡å‹...")
        model, tokenizer = loader.load_teacher_model()
    else:
        print("âŒ æ— æ•ˆé€‰æ‹©")
        return
    
    if model is None:
        print("âŒ æ¨¡å‹åŠ è½½å¤±è´¥")
        return
    
    # äº¤äº’å¼é—®ç­”
    print("\nğŸ’¬ å¼€å§‹äº¤äº’ (è¾“å…¥ 'quit' é€€å‡º):")
    while True:
        user_input = input("\nğŸ™‹ æ‚¨çš„é—®é¢˜: ").strip()
        
        if user_input.lower() in ['quit', 'exit', 'é€€å‡º']:
            break
        
        if not user_input:
            continue
        
        # æ ¼å¼åŒ–è¾“å…¥
        prompt = f"""### æŒ‡ä»¤:
åˆ†æä»£ç å¹¶æä¾›å»ºè®®

### è¾“å…¥:
{user_input}

### è¾“å‡º:
"""
        
        try:
            # æ¨ç†
            device = next(model.parameters()).device
            inputs = tokenizer(prompt, return_tensors="pt")
            inputs = {k: v.to(device) for k, v in inputs.items()}
            
            print("ğŸ¤– æ€è€ƒä¸­...")
            import torch
            with torch.no_grad():
                outputs = model.generate(
                    **inputs,
                    max_new_tokens=config.get("inference_config", {}).get("max_new_tokens", 200),
                    temperature=config.get("inference_config", {}).get("temperature", 0.7),
                    do_sample=True,
                    pad_token_id=tokenizer.eos_token_id
                )
            
            response = tokenizer.decode(outputs[0][inputs['input_ids'].shape[1]:], skip_special_tokens=True)
            
            print(f"ğŸ¤– å›ç­”:\n{response}")
            
        except Exception as e:
            print(f"âŒ æ¨ç†å¤±è´¥: {e}")
    
    print("\nğŸ‘‹ å†è§!")

if __name__ == "__main__":
    main()
