"""
æœ¬åœ°æ¨¡å‹åŠ è½½å™¨ - åœ¨æœ¬æœºä½¿ç”¨äº‘ç«¯ä¸‹è½½çš„æ¨¡å‹
"""
import os
import torch
from pathlib import Path
from transformers import (
    AutoTokenizer, 
    AutoModelForCausalLM,
    BitsAndBytesConfig
)
from peft import PeftModel
import json

class LocalModelLoader:
    def __init__(self, cloud_data_dir=None):
        if cloud_data_dir is None:
            # é»˜è®¤ä½¿ç”¨é¡¹ç›®æ ¹ç›®å½•ä¸‹çš„cloud_data
            project_root = Path(__file__).parent
            cloud_data_dir = project_root / "cloud_data"
        
        self.cloud_data_dir = Path(cloud_data_dir)
        self.models_dir = self.cloud_data_dir / "models"
        
        # æ£€æŸ¥ç›®å½•æ˜¯å¦å­˜åœ¨
        if not self.cloud_data_dir.exists():
            raise FileNotFoundError(f"cloud_dataç›®å½•ä¸å­˜åœ¨: {self.cloud_data_dir}")
    
    def load_teacher_model(self, use_quantization=True):
        """åŠ è½½7Bæ•™å¸ˆæ¨¡å‹ (LoRAå¾®è°ƒç‰ˆ)"""
        print("ğŸ“ åŠ è½½7Bæ•™å¸ˆæ¨¡å‹...")
        
        teacher_path = self.models_dir / "teacher_7b"
        if not teacher_path.exists():
            raise FileNotFoundError(f"æ•™å¸ˆæ¨¡å‹ç›®å½•ä¸å­˜åœ¨: {teacher_path}")
        
        try:
            # åŠ è½½åˆ†è¯å™¨
            tokenizer = AutoTokenizer.from_pretrained(
                "Qwen/Qwen1.5-7B-Chat",
                trust_remote_code=True
            )
            
            # é…ç½®é‡åŒ– (å¯é€‰)
            quantization_config = None
            if use_quantization:
                quantization_config = BitsAndBytesConfig(
                    load_in_8bit=True,
                    bnb_8bit_compute_dtype=torch.float16
                )
            
            # åŠ è½½åŸºç¡€æ¨¡å‹
            base_model = AutoModelForCausalLM.from_pretrained(
                "Qwen/Qwen1.5-7B-Chat",
                quantization_config=quantization_config,
                device_map="auto",
                trust_remote_code=True
            )
            
            # åŠ è½½LoRAé€‚é…å™¨
            teacher_model = PeftModel.from_pretrained(base_model, teacher_path)
            
            print("âœ… 7Bæ•™å¸ˆæ¨¡å‹åŠ è½½æˆåŠŸ")
            return teacher_model, tokenizer
            
        except Exception as e:
            print(f"âŒ æ•™å¸ˆæ¨¡å‹åŠ è½½å¤±è´¥: {e}")
            return None, None
    
    def load_student_model(self, use_quantization=True):
        """åŠ è½½0.5Bå­¦ç”Ÿæ¨¡å‹ (LoRAè’¸é¦ç‰ˆ)"""
        print("ğŸ‘¨â€ğŸ“ åŠ è½½0.5Bå­¦ç”Ÿæ¨¡å‹...")
        
        student_path = self.models_dir / "student_0.5b"
        if not student_path.exists():
            raise FileNotFoundError(f"å­¦ç”Ÿæ¨¡å‹ç›®å½•ä¸å­˜åœ¨: {student_path}")
        
        try:
            # åŠ è½½åˆ†è¯å™¨
            tokenizer = AutoTokenizer.from_pretrained(
                "Qwen/Qwen1.5-0.5B-Chat",
                trust_remote_code=True
            )
            
            # é…ç½®é‡åŒ– (å¯é€‰)
            quantization_config = None
            if use_quantization:
                quantization_config = BitsAndBytesConfig(
                    load_in_8bit=True,
                    bnb_8bit_compute_dtype=torch.float16
                )
            
            # åŠ è½½åŸºç¡€æ¨¡å‹
            base_model = AutoModelForCausalLM.from_pretrained(
                "Qwen/Qwen1.5-0.5B-Chat",
                quantization_config=quantization_config,
                device_map="auto",
                trust_remote_code=True
            )
            
            # åŠ è½½LoRAé€‚é…å™¨
            student_model = PeftModel.from_pretrained(base_model, student_path)
            
            print("âœ… 0.5Bå­¦ç”Ÿæ¨¡å‹åŠ è½½æˆåŠŸ")
            return student_model, tokenizer
            
        except Exception as e:
            print(f"âŒ å­¦ç”Ÿæ¨¡å‹åŠ è½½å¤±è´¥: {e}")
            return None, None
    
    def load_teacher_knowledge(self):
        """åŠ è½½æ•™å¸ˆçŸ¥è¯†æ–‡ä»¶"""
        print("ğŸ“– åŠ è½½æ•™å¸ˆçŸ¥è¯†...")
        
        knowledge_path = self.cloud_data_dir / "knowledge" / "teacher_knowledge_lite.pkl"
        if not knowledge_path.exists():
            raise FileNotFoundError(f"æ•™å¸ˆçŸ¥è¯†æ–‡ä»¶ä¸å­˜åœ¨: {knowledge_path}")
        
        try:
            import pickle
            with open(knowledge_path, 'rb') as f:
                knowledge_data = pickle.load(f)
            
            print(f"âœ… æ•™å¸ˆçŸ¥è¯†åŠ è½½æˆåŠŸ: {len(knowledge_data['teacher_knowledge'])} ä¸ªæ ·æœ¬")
            return knowledge_data
            
        except Exception as e:
            print(f"âŒ æ•™å¸ˆçŸ¥è¯†åŠ è½½å¤±è´¥: {e}")
            return None
    
    def get_model_info(self):
        """è·å–æ¨¡å‹ä¿¡æ¯"""
        info = {
            "cloud_data_dir": str(self.cloud_data_dir.absolute()),
            "models": {},
            "files": {}
        }
        
        # æ£€æŸ¥æ¨¡å‹ç›®å½•
        teacher_path = self.models_dir / "teacher_7b"
        student_path = self.models_dir / "student_0.5b"
        knowledge_path = self.cloud_data_dir / "knowledge" / "teacher_knowledge_lite.pkl"
        data_path = self.cloud_data_dir / "data" / "training_data.jsonl"
        
        info["models"]["teacher_7b"] = {
            "path": str(teacher_path),
            "exists": teacher_path.exists(),
            "size": self._get_dir_size(teacher_path) if teacher_path.exists() else "N/A"
        }
        
        info["models"]["student_0.5b"] = {
            "path": str(student_path),
            "exists": student_path.exists(),
            "size": self._get_dir_size(student_path) if student_path.exists() else "N/A"
        }
        
        info["files"]["teacher_knowledge"] = {
            "path": str(knowledge_path),
            "exists": knowledge_path.exists(),
            "size": self._get_file_size(knowledge_path) if knowledge_path.exists() else "N/A"
        }
        
        info["files"]["training_data"] = {
            "path": str(data_path),
            "exists": data_path.exists(),
            "size": self._get_file_size(data_path) if data_path.exists() else "N/A"
        }
        
        return info
    
    def _get_dir_size(self, directory):
        """è·å–ç›®å½•å¤§å°"""
        try:
            total_size = 0
            for dirpath, dirnames, filenames in os.walk(directory):
                for filename in filenames:
                    filepath = os.path.join(dirpath, filename)
                    total_size += os.path.getsize(filepath)
            return f"{total_size / 1024**2:.1f}MB"
        except:
            return "æœªçŸ¥"
    
    def _get_file_size(self, filepath):
        """è·å–æ–‡ä»¶å¤§å°"""
        try:
            size = os.path.getsize(filepath)
            if size > 1024**3:
                return f"{size / 1024**3:.1f}GB"
            else:
                return f"{size / 1024**2:.1f}MB"
        except:
            return "æœªçŸ¥"

def demo_usage():
    """æ¼”ç¤ºå¦‚ä½•ä½¿ç”¨æœ¬åœ°æ¨¡å‹åŠ è½½å™¨"""
    print("ğŸš€ æœ¬åœ°æ¨¡å‹åŠ è½½å™¨æ¼”ç¤º")
    print("=" * 50)
    
    try:
        # åˆ›å»ºåŠ è½½å™¨
        loader = LocalModelLoader()
        
        # è·å–æ¨¡å‹ä¿¡æ¯
        info = loader.get_model_info()
        print("ğŸ“Š æ¨¡å‹çŠ¶æ€:")
        print(json.dumps(info, indent=2, ensure_ascii=False))
        
        # æµ‹è¯•åŠ è½½å­¦ç”Ÿæ¨¡å‹ (æ›´è½»é‡)
        print("\nğŸ§ª æµ‹è¯•åŠ è½½0.5Bå­¦ç”Ÿæ¨¡å‹...")
        student_model, tokenizer = loader.load_student_model()
        
        if student_model and tokenizer:
            # æµ‹è¯•æ¨ç†
            test_prompt = """### æŒ‡ä»¤:
åˆ†æä»£ç å¹¶æä¾›ä¼˜åŒ–å»ºè®®

### è¾“å…¥:
def slow_search(arr, target):
    for i in range(len(arr)):
        if arr[i] == target:
            return i
    return -1

### è¾“å‡º:
"""
            
            print("ğŸ”¬ æµ‹è¯•æ¨ç†...")
            device = next(student_model.parameters()).device
            inputs = tokenizer(test_prompt, return_tensors="pt")
            inputs = {k: v.to(device) for k, v in inputs.items()}
            
            with torch.no_grad():
                outputs = student_model.generate(
                    **inputs,
                    max_new_tokens=150,
                    temperature=0.7,
                    do_sample=True,
                    pad_token_id=tokenizer.eos_token_id
                )
            
            response = tokenizer.decode(outputs[0][inputs['input_ids'].shape[1]:], skip_special_tokens=True)
            
            print("ğŸ“ æ¨¡å‹è¾“å‡º:")
            print("-" * 40)
            print(response)
            print("-" * 40)
            print("âœ… æœ¬åœ°æ¨¡å‹æµ‹è¯•æˆåŠŸ!")
        
    except Exception as e:
        print(f"âŒ æ¼”ç¤ºå¤±è´¥: {e}")
        print("\nğŸ’¡ å¯èƒ½çš„åŸå› :")
        print("1. cloud_dataæ–‡ä»¶å¤¹ä¸å­˜åœ¨")
        print("2. æ¨¡å‹æ–‡ä»¶æœªä¸‹è½½")
        print("3. ä¾èµ–åº“æœªå®‰è£…")
        print("\nğŸ”§ è§£å†³æ–¹æ¡ˆ:")
        print("1. è¿è¡Œ python create_cloud_data_folder.py")
        print("2. è¿è¡Œ python download_cloud_models.py")
        print("3. å®‰è£…ä¾èµ–: pip install transformers peft bitsandbytes")

if __name__ == "__main__":
    demo_usage()
